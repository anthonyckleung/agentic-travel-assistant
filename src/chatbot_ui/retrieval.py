from openai import OpenAI
import instructor
import psycopg2
import sqlvalidator
# from qdrant_client import QdrantClient
# from langsmith import traceable, get_current_run_tree
from pydantic import BaseModel
from core.config import config


class SQLQuery(BaseModel):
    query: str


class RAGGenerationResponse(BaseModel):
    answer: str


client = instructor.from_openai(OpenAI(api_key=config.OPENAI_API_KEY))


def is_valid_sql(query):
    parsed = sqlvalidator.parse(query)
    return parsed.is_valid()


def build_sql_generate_prompt(user_query):
    prompt = f"""
    You are given a database schema:
        Schema: public
        Table: us_attractions
        Columns:
        - name VARCHAR(250)
        - main_category VARCHAR(250)
        - rating REAL
        - reviews REAL
        - categories VARCHAR(250)
        - address VARCHAR(250)
        - city VARCHAR(250)
        - country VARCHAR(250)
        - state VARCHAR(250)
        - zipcode INTEGER
        - broader_category VARCHAR(250)
        - weighted_score REAL
        - weighted_average REAL
        - all_cities VARCHAR(250)

    The values under the country column are all "USA".    

    Translate the following user question into SQL query statement:

    "{user_query}"

    Write SQL query using the "public" schema for all tables (e.g., public.us_attraction).
    """
    return prompt


def build_rag_response_prompt(rows, user_question, sql_query):
    formatted_rows = "\n".join([", ".join(map(str, row)) for row in rows])
    # print(formatted_rows)

    # Create a prompt for the LLM
    prompt = f'''Here are the SQL query results:\n{formatted_rows}

    Generated by this SQL query: {sql_query}\n
    '''
    if user_question:
        prompt += f"Based on these results, answer the question: {user_question}"

    return prompt


def generate_sql_query(prompt):
    response, _ = client.chat.completions.create_with_completion(
        model="gpt-4.1-mini",
        response_model=SQLQuery,
        messages=[{"role":"user", "content": prompt}],
        temperature=0
    )
    return response.query


def retrieve_from_postgres(cursor, sql_query):
    cursor.execute(sql_query)
    rows = cursor.fetchall()
    return rows


def generate_answer(prompt):
    response, _ = client.chat.completions.create_with_completion(
        model="gpt-4.1-mini",
        response_model=RAGGenerationResponse,
        messages=[{"role":"user", "content": prompt}],
        temperature=0.2
    )
    return response.answer


def rag_pipeline(user_question, psycopg_cursor):
    sql_prompt = build_sql_generate_prompt(user_question)
    sql_query = generate_sql_query(sql_prompt)
    # print("sql_query")
    if is_valid_sql(sql_query):
        query_result = retrieve_from_postgres(psycopg_cursor, sql_query)
        response_prompt = build_rag_response_prompt(query_result, user_question, sql_query)
        answer = generate_answer(response_prompt)
    else:
        answer = sql_query
    
    final_result = {
        "answer": answer,
        "question": user_question,
    }
    return final_result
